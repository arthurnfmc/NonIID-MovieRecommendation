# -*- coding: utf-8 -*-
"""CompartilhamentoParcialDadosExperimento-AprendizagemFederada_nao_iid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RnATI3l0mAFyxXZNcDApNz_otfXdiwod
"""

# @title Bibliotecas

import pandas as pd
import numpy as np
import flwr as fl
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow import keras
from tensorflow.keras import layers
from flwr_datasets.partitioner import PathologicalPartitioner, IidPartitioner
from datasets import Dataset
import time

# @title Leitura dos dados
links = pd.read_csv('ml-latest-small/links.csv')
movies = pd.read_csv('ml-latest-small/movies.csv')
ratings = pd.read_csv('ml-latest-small/ratings.csv')

#"""### Visualização prévia dos dados"""
#
#links.head()
#
#movies.head()
#
#ratings.head()
#
#data.head()

"""### Manipulação dos dados

"""

data = links.merge(movies, on="movieId").merge(ratings, on="movieId")
data['genres'] = data['genres'].str.split('|')

# Normalização
data['timestamp'] = (data['timestamp'] - data['timestamp'].min()) / (data['timestamp'].max() - data['timestamp'].min())
data['userId'] = (data['userId'] - data['userId'].min()) / (data['userId'].max() - data['userId'].min())
data['movieId'] = (data['movieId'] - data['movieId'].min()) / (data['movieId'].max() - data['movieId'].min())

data = data.drop(columns=['imdbId', 'tmdbId', 'title'])

generos_explodidos = data.explode('genres')

# Criar colunas binárias para cada gênero
generos_binarios = pd.get_dummies(generos_explodidos['genres'], dtype=int)

# Reagrupar os dados ao nível do filme (somando as colunas binárias)
generos_binarios_agrupados = generos_binarios.groupby(generos_explodidos.index).max()

# Concatenar as colunas binárias de volta ao DataFrame original
df = pd.concat([data.drop(columns='genres'), generos_binarios_agrupados], axis=1)
df = df.drop(columns='(no genres listed)')

#print(df)

def main():
  """## Experimentação de Fato - Com dados compartilhados"""

  # @title Separação Treino/Teste (o teste será federado)

  train, test = train_test_split(df, test_size=0.3)

  # @title Código responsavel por criar/carregar partições

  # Partitioner para o teste federado
  test_partitioner = IidPartitioner(num_partitions=204)
  test_fds = Dataset.from_pandas(test)
  test_partitioner.dataset = test_fds

  # Carregar uma particao de teste
  def load_test_partition(partition_id):
    partition_test = test_partitioner.load_partition(partition_id=partition_id)
    partition_np = partition_test.with_format("numpy")

    # Converter para numpy
    X = partition_np[list(partition_np.features.keys())[0]]
    for feature in list(partition_np.features.keys())[1:]:
      if feature != 'rating' and feature != '__index_level_0__':
        X = np.column_stack((X, partition_np[feature]))

    y = partition_np['rating']
    return X, y

  # Partitioner que separa por userId.
  # Treinamento será feito com 610 nós cada um conhecendo os reviews de apenas 1 clientes
  pathological_partitioner = PathologicalPartitioner(
      num_partitions=204, partition_by="userId", num_classes_per_partition=3
  )

  # Carregando os dados
  fds = Dataset.from_pandas(train)
  pathological_partitioner.dataset = fds

  ## Criando o subset compartilhado
  #x_shared = None
  #y_shared = None
  #for i in range(204):
  #  partition_np = pathological_partitioner.load_partition(partition_id=i).with_format("numpy")
#
  #  # Converter para numpy
  #  X = partition_np[list(partition_np.features.keys())[0]]
  #  for feature in list(partition_np.features.keys())[1:]:
  #    if feature != 'rating' and feature != '__index_level_0__':
  #      X = np.column_stack((X, partition_np[feature]))
  #  y = partition_np['rating']
#
  #  length = int(len(X)*0.05)
  #  if i==0:
  #    x_shared = X[:length]
  #    y_shared = y[:length]
  #  else:
  #    x_shared = np.concatenate((x_shared, X[:length]), axis=0)
  #    y_shared = np.concatenate((y_shared, y[:length]), axis=0)

  # Carregar uma particao
  #def load_partition(partition_id):
  #  partition_pathological = pathological_partitioner.load_partition(partition_id=partition_id)
  #  partition_np = partition_pathological.with_format("numpy")
#
  #  # Converter para numpy
  #  X = partition_np[list(partition_np.features.keys())[0]]
  #  for feature in list(partition_np.features.keys())[1:]:
  #    if feature != 'rating' and feature != '__index_level_0__':
  #      X = np.column_stack((X, partition_np[feature]))
#
  #  y = partition_np['rating']
  #  length = int(len(X)*0.05)
  #  return np.concatenate((X[length:], x_shared), axis=0), np.concatenate((y[length:], y_shared), axis=0)
  
  # Particoes sem dados compartilhados
  #def load_partition(partition_id):
  #  partition_pathological = pathological_partitioner.load_partition(partition_id=partition_id)
  #  partition_np = partition_pathological.with_format("numpy")
  #
  #  # Converter para numpy
  #  X = partition_np[list(partition_np.features.keys())[0]]
  #  for feature in list(partition_np.features.keys())[1:]:
  #    if feature != 'rating' and feature != '__index_level_0__':
  #      X = np.column_stack((X, partition_np[feature]))
  #
  #  y = partition_np['rating']
  #  return X, y

  # Partição IID (VALOR DE REFERENCIA)
  # Partitioner para o teste federado
  iid_partitioner = IidPartitioner(num_partitions=204)
  iid_fds = Dataset.from_pandas(train)
  iid_partitioner.dataset = iid_fds

  # Carregar uma particao de treino IID (VALOR DE REFERENCIA)
  def load_partition(partition_id):
    partition_iid = iid_partitioner.load_partition(partition_id=partition_id)
    partition_np = partition_iid.with_format("numpy")

    # Converter para numpy
    X = partition_np[list(partition_np.features.keys())[0]]
    for feature in list(partition_np.features.keys())[1:]:
      if feature != 'rating' and feature != '__index_level_0__':
        X = np.column_stack((X, partition_np[feature]))

    y = partition_np['rating']
    return X, y

  # Média ponderada
  def weighted_average(metrics):
      maes = [num_examples * m["mae"] for num_examples, m in metrics]
      mses = [num_examples * m["mse"] for num_examples, m in metrics]
      examples = [num_examples for num_examples, _ in metrics]

      # Aggregate and return custom metric (weighted average)
      return {"maes": sum(maes) / sum(examples),
              "mses": sum(mses) / sum(examples)}

  # Classe do Cliente Flower
  class MovieLensClient(fl.client.NumPyClient):
      def __init__(self, x_train, y_train, x_val, y_val):
          self.model = keras.Sequential([
              layers.Dense(64, activation='relu', input_shape=(x_train.shape[1],)),
              layers.Dense(32, activation='relu'),
              layers.Dense(1)
          ])
          self.model.compile(optimizer='adam', loss='mse', metrics=['mae'])
          self.x_train, self.y_train = x_train, y_train
          self.x_val, self.y_val = x_val, y_val

      def get_parameters(self, config):
          return self.model.get_weights()

      def set_parameters(self, parameters):
          self.model.set_weights(parameters)

      def fit(self, parameters, config):
          self.model.set_weights(parameters)
          self.model.fit(self.x_train, self.y_train, epochs=5, verbose=1)
          return self.model.get_weights(), len(self.x_train), {}

      def evaluate(self, parameters, config):
          self.model.set_weights(parameters)
          loss, mae = self.model.evaluate(self.x_val, self.y_val, verbose=1)
          return loss, len(self.x_val), {"mae": mae, "mse": loss}

  # Gerador de clientes
  def client_fn(cid):
    X, y = load_partition(int(cid))
    x_val, y_val = load_test_partition(int(cid))
    
    #x_val = val.drop(columns=['rating']).to_numpy()
    #y_val = val['rating'].to_numpy()

    return MovieLensClient(X, y, x_val, y_val)

  strategy = fl.server.strategy.FedAvg(
      fraction_fit=1,  # 100% dos clientes para treino
      fraction_evaluate=1, # 100% dos clientes para teste
      min_fit_clients=204,
      min_evaluate_clients=204,
      min_available_clients=204,
      evaluate_metrics_aggregation_fn=weighted_average,
  )

  # Dados do treinamento sao salvos em 'history'
  history = fl.simulation.start_simulation(
      client_fn=client_fn,
      num_clients=204,
      config=fl.server.ServerConfig(num_rounds=3),
      strategy=strategy,
      #client_resources={"num_cpus": 1, "num_gpus": 1}
  )

  print(history)
  print(history.metrics_distributed)
  return history

N = 30 # Numero de execucoes
REPORT_F_PATH = "./resultado_n=30_IID-REFERENCIA_fed-eval.csv" #"./resultado_n=30_sem-dados-compartilhados_fed-eval.csv" #"./resultado_n=30_com-dados-compartilhados_fed-eval.csv"
with open(REPORT_F_PATH, "a+") as f:
  f.write("i,mae,mse,tempo_em_segundos\n")

for i in range(N):
  # Execucao
  start = time.time()
  history = main()
  end = time.time()

  # Reports
  mae = history.metrics_distributed['maes'][-1][1]
  mse = history.metrics_distributed['mses'][-1][1]
  with open(REPORT_F_PATH, "a+") as f:
    f.write(f"{i},{mae},{mse},{end-start}\n")


"""## Experimentação - Sem dados compartilhados"""

## @title Separação Treino/Validação/Teste
#
#train, test = train_test_split(df, test_size=0.3, random_state=42)
#val, test = train_test_split(test, test_size=0.666, random_state=42)
#
## @title Código responsavel por criar/carregar partições
#
#from flwr_datasets.partitioner import PathologicalPartitioner
#from datasets import Dataset
#
## Partitioner que separa por userId.
## Treinamento será feito com X nós cada um conhecendo os reviews de apenas Y clientes
#pathological_partitioner = PathologicalPartitioner(
#    num_partitions=204, partition_by="userId", num_classes_per_partition=3
#)
#
## Carregando os dados
#fds = Dataset.from_pandas(train)
#pathological_partitioner.dataset = fds
#
## Carregar uma particao
#def load_partition(partition_id):
#  partition_pathological = pathological_partitioner.load_partition(partition_id=partition_id)
#  partition_np = partition_pathological.with_format("numpy")
#
#  # Converter para numpy
#  X = partition_np[list(partition_np.features.keys())[0]]
#  for feature in list(partition_np.features.keys())[1:]:
#    if feature != 'rating' and feature != '__index_level_0__':
#      X = np.column_stack((X, partition_np[feature]))
#
#  y = partition_np['rating']
#  return X, y
#
## Média ponderada
#def weighted_average(metrics):
#    maes = [num_examples * m["mae"] for num_examples, m in metrics]
#    mses = [num_examples * m["mse"] for num_examples, m in metrics]
#    examples = [num_examples for num_examples, _ in metrics]
#
#    # Aggregate and return custom metric (weighted average)
#    return {"mae": sum(maes) / sum(examples),
#             "mses": sum(mses) / sum(examples)}
#
## Classe do Cliente Flower
#class MovieLensClient(fl.client.NumPyClient):
#    def __init__(self, x_train, y_train, x_val, y_val):
#        self.model = keras.Sequential([
#            layers.Dense(64, activation='relu', input_shape=(x_train.shape[1],)),
#            layers.Dense(32, activation='relu'),
#            layers.Dense(1)
#        ])
#        self.model.compile(optimizer='adam', loss='mse', metrics=['mae'])
#        self.x_train, self.y_train = x_train, y_train
#        self.x_val, self.y_val = x_val, y_val
#
#    def get_parameters(self, config):
#        return self.model.get_weights()
#
#    def set_parameters(self, parameters):
#        self.model.set_weights(parameters)
#
#    def fit(self, parameters, config):
#        self.model.set_weights(parameters)
#        self.model.fit(self.x_train, self.y_train, epochs=5, verbose=1)
#        return self.model.get_weights(), len(self.x_train), {}
#
#    def evaluate(self, parameters, config):
#        self.model.set_weights(parameters)
#        loss, mae = self.model.evaluate(self.x_val, self.y_val, verbose=1)
#        return loss, len(self.x_val), {"mae": mae, "mse": loss}
#
## Gerador de clientes
#def client_fn(cid):
#  X, y = load_partition(int(cid))
#
#  x_val = val.drop(columns=['rating']).to_numpy()
#  y_val = val['rating'].to_numpy()
#
#  return MovieLensClient(X, y, x_val, y_val)
#
## Start Flower simulation
#strategy = fl.server.strategy.FedAvg(
#    fraction_fit=1,  # 100% dos clientes para treino
#    fraction_evaluate=1, # 100% dos clientes para teste
#    min_fit_clients=204,
#    min_evaluate_clients=204,
#    min_available_clients=204,
#    evaluate_metrics_aggregation_fn=weighted_average
#)
#
## Dados do treinamento sao salvos em 'history'
#history = fl.simulation.start_simulation(
#    client_fn=client_fn,
#    num_clients=204,
#    config=fl.server.ServerConfig(num_rounds=3),
#    strategy=strategy
#)
#
#print(history.metrics_distributed)

#"""### XGBoost"""
#
## prompt: troque a arquitetura de rede neural para uma xgboost
#
#import pandas as pd
#import numpy as np
#import flwr as fl
#from sklearn.model_selection import train_test_split
#from sklearn.preprocessing import StandardScaler
#from xgboost import XGBRegressor
#from flwr_datasets.partitioner import PathologicalPartitioner
#from datasets import Dataset
#
## Download dos dados (se ainda não estiverem disponíveis)
#!wget https://files.grouplens.org/datasets/movielens/ml-latest-small.zip
#!unzip ml-latest-small.zip
#!pip install -q "flwr-datasets[vision]"
#!pip install flwr['simulation']
#!pip install xgboost
#
#
## Leitura dos dados
#links = pd.read_csv('ml-latest-small/links.csv')
#movies = pd.read_csv('ml-latest-small/movies.csv')
#ratings = pd.read_csv('ml-latest-small/ratings.csv')
#
## Manipulação dos dados (mesmo pré-processamento)
#data = links.merge(movies, on="movieId").merge(ratings, on="movieId")
#data['genres'] = data['genres'].str.split('|')
#data['timestamp'] = (data['timestamp'] - data['timestamp'].min()) / (data['timestamp'].max() - data['timestamp'].min())
#data = data.drop(columns=['imdbId', 'tmdbId', 'title'])
#
#generos_explodidos = data.explode('genres')
#generos_binarios = pd.get_dummies(generos_explodidos['genres'], dtype=int)
#generos_binarios_agrupados = generos_binarios.groupby(generos_explodidos.index).max()
#df = pd.concat([data.drop(columns='genres'), generos_binarios_agrupados], axis=1)
#df = df.drop(columns='(no genres listed)')
#
## Separar features e labels
#X = df.drop(columns=['rating'])
#y = df['rating']
#
## Train/test split e escalonamento
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
#scaler = StandardScaler()
#X_train = scaler.fit_transform(X_train)
#X_test = scaler.transform(X_test)
#
#
## Classe do Cliente Flower para XGBoost
#class MovieLensClient(fl.client.NumPyClient):
#    def __init__(self, x_train, y_train, x_val, y_val):
#        self.model = XGBRegressor(objective='reg:squarederror', n_estimators=10) # Modelo XGBoost
#        self.x_train, self.y_train = x_train, y_train
#        self.x_val, self.y_val = x_val, y_val
#
#    def get_parameters(self, config):
#      return self.model.get_xgb_params()
#
#    def set_parameters(self, parameters):
#      self.model.set_params(**parameters)
#
#    def fit(self, parameters, config):
#      self.set_parameters(parameters)
#      self.model.fit(self.x_train, self.y_train)
#      return self.model.get_xgb_params(), len(self.x_train), {}
#
#    def evaluate(self, parameters, config):
#      self.set_parameters(parameters)
#      predictions = self.model.predict(self.x_val)
#      mae = np.mean(np.abs(predictions - self.y_val))
#      mse = np.mean((predictions - self.y_val)**2)
#      return mse, len(self.x_val), {"mae": mae, "mse": mse}
#
#
## ... (restante do código para definir client_fn, strategy e iniciar a simulação)
#
## ... (código para iniciar a simulação, idêntico ao original)
## Dados do treinamento sao salvos em 'history'
#history = fl.simulation.start_simulation(
#    client_fn=client_fn,
#    num_clients=2,
#    config=fl.server.ServerConfig(num_rounds=3),
#    strategy=strategy
#)
#history.metrics_distributed